# Probabilistic model

## 概率图模型

机器学习最重要的任务，是根据一些已观察到的证据（例如训练样本）来对感兴趣的未知变量（例如类别标记）进行估计和推测。概率模型(probabilistic model) 提供了一种描述框架，将学习任务归结于计算变量的概率分布。在概率模型中，利用已知变量推测未知变量的分布称为 “推断(inference)”，其核心是如何基于可观测变量推测出未知变量的条件分布。

具体来说，假定所关心的变量集合为 $Y$，可观测变量集合为 $O$，其他变量集合为 $R$，“生成式(generative)” 模型考虑联合分布 $P(Y, R, O)$，“判别式(discriminative)” 模型考虑条件分布 $P(Y, R | O)$。给定一组观测变量值，推断(inference)就是要由 $P(Y, R, O)$ 或 $P(Y, R | O)$ 得到条件概率分布 $P(Y | O)$。

概率模型的学习，即基于训练样本来估计变量分布的参数相当困难[^1]。为了便于研究高效的推断和学习算法，需要有一套简洁紧凑地表达变量间关系的工具。

概率图模型(probabilistic graphical model) 是一类用图来表达变量*相关*关系的概率模型。它以图为表示工具，最常用的是一个结点表示一个或一组随机变量，结点之间的边表示变量间的相关关系，即 “变量关系图”。根据边的性质不同，概率图模型可大致分为两类：第一类是使用有向无环图表示变量间的依赖关系，称为 “有向图模型” 或 “贝叶斯网(Bayesian network)”；第二类是使用无向图表示变量间的相关关系，称为 “无向图模型” 或 “马尔可夫网(Markov network)”。

### 隐马尔可夫模型

隐马尔可夫模型(Hidden Markov Model, HMM) 是结构最简单的动态贝叶斯网(dynamic Bayesian network)，这是一种著名的有向图模型，主要用于时序数据建模，在语音识别、自然语言处理等领域有广泛应用。

![hmm1](./images/ml_hmm_structure.png)

如图14.1所示，隐马尔可夫模型中的变量可分为两组。第一组是状态变量 $\{y_1, \ldots, y_n \}$，其中 $y_i \in \mathcal{Y}$ 表示第 $i$ 时刻的系统状态。通常假定状态变量是 “隐藏的、不可观测的”，因此状态变量也称为 “隐变量(hidden variable)”。第二组是观测变量 $\{x_1, \ldots, x_n \}$，其中 $x_i \in \mathcal{X}$ 表示第 $i$ 时刻的观测值。在隐马尔可夫模型中，系统通常在多个状态 $\{s_1, \ldots, s_N \}$ 之间转换，因此状态变量 $y_i$ 的取值范围 $\mathcal{Y}$ (称为状态空间)通常是有 $N$ 个可能取值的离散空间。观测变量 $x_i$ 可以是离散的也可以是连续的，为便于讨论，我们仅考虑离散型观测变量，并假定其取值范围 $\cal{X}$ 为 $\{o_1, \ldots, o_M \}$。

图14.1中的箭头表示了变量间的依赖关系。在任一时刻，观测变量的取值仅依赖于状态变量，即 $x_t$ 由 $y_t$ 确定，与其他状态变量及观测变量的取值无关。同时，$t$ 时刻的状态 $y_t$ 仅依赖于 $t-1$ 时刻的状态 $y_{t-1}$，与其余 $n-2$ 个状态无关。这就是所谓的 “马尔可夫链(Markov chain)”，即：系统下一时刻的状态仅由当前状态决定，不依赖于以往的任何状态。基于这种依赖关系，所有变量的联合概率分布为
$$
\tag{14.1}
P(x_1,y_1, \ldots, x_n,y_n) = P(y_1) P(x1 | y_1) \prod^n_{i=2} P(y_i) P(x_i | y_i).
$$
除了结构信息，欲确定一个隐马尔可夫模型还需要以下三组参数：

- 状态转移概率：模型在各个状态间转换的概率，通常记为矩阵 $A = [a_{ij}]_{N \times N}$，其中
  $$
  a_{ij} = P(y_{t+1} = s_j | y_t = s_i), \qquad 1 \le i,j \le N,
  $$
  表示在任意时刻 $t$，若状态为 $s_i$，则下一时刻状态为 $s_j$ 的概率。

- 输出观测概率：模型根据当前状态获得各个观测值的概率，通常记为矩阵 $B = [b_{ij}]_{N \times M}$，其中
  $$
  b_{ij} = P(x_t = o_j | y_t = s_i), \qquad {1 \le i \le N},\ {1 \le j \le M}
  $$
  表示在任意时刻 $t$，若状态为 $s_i$，则观测值 $o_j$ 被获取的概率。

- 初始状态概率：模型在初始时刻各状态出现的概率，通常记为 $\pi = (\pi_1, \ldots, \pi_N)$，其中
  $$
  \pi_i = P(y_1 = s_i), \qquad {1 \le i \le N}
  $$
  表示模型的初始状态为 $s_i$ 的概率。

通过指定状态空间 $\cal{Y}$ 、观测空间 $\cal{X}$ 和上述三组参数，就能确定一个隐马尔可夫模型，通常其参数 $\lambda = [A, B, \pi]$ 来指代。给定隐马尔可夫模型 $\lambda$ ，它按如下过程产生观测序列 $\{x_1, \ldots, x_n \}$ :

1. 设置 $t = 1$ ，并根据初始状态概率 $\pi$ 选择初始状态 $y_i$;
2. 根据状态 $y_t$ 和输出观测概率 $B$ 选择观测变量取值 $x_t$ ;
3. 根据状态 $y_t$ 和状态转移矩阵 $A$ 转移模型状态，即确定 $y_{t+1}$ ；
4. 若 $t < n$ ，设置 $t = t + 1$ ，并转到第 2. 步，否则停止。

其中，$y_t \in \{s_1, \ldots, s_N \}$ 和 $x_t \in \{o_1, \ldots, o_M \}$ 分别为第 $t$ 时刻的状态和观测值。

在实际应用中，人们通常关注隐马尔可夫模型的三个基本问题：

- 给定模型 $\lambda = [A, B, \pi]$ ，如何有效计算其产生观测序列 $\mathbf{x} = \{x_1, \ldots, x_n \}$ 的概率 $P(\bf{x} | \lambda)$ ？换言之，如何评估模型与观测序列之间的匹配程度？
- 给定模型 $\lambda = [A, B, \pi]$ 和观测序列 $\mathbf{x} = \{x_1, \ldots, x_n \}$ ，如何找到与此观测序列最匹配的状态序列 $\mathbf{y} = \{y_1, \ldots, y_n \}$ ？换言之，如何根据观测序列推断出隐藏的模型状态？
- 给定观测序列 $\mathbf{x} = \{x_1, \ldots, x_n \}$ ，如何模型参数 $\lambda = [A, B, \pi]$ 使得该序列出现的概率 $P(\bf{x} | \lambda)$ 最大？换言之，如何训练模型使其能最好地描述观测数据？

上述问题在现实应用中非常重要。例如许多任务需要根据以往的观测序列 $\{x_1, \ldots, x_n \}$ 来推测当前时刻最有可能的观测值 $x_n$ ，这显然可转化为求取概率  $P(\bf{x} | \lambda)$ ，即上述第一个问题；在语音识别等任务中，观测值为语音信号，隐藏状态为文字，目标就是根据观测信号来推断最有可能的状态序列（即对应的文字），也就是上述第二个问题；在大多数现实应用中，人工指定模型参数已变得越来越不可行，如何根据训练样本学得最优的模型参数，恰是上述第三个问题。值的庆幸的是，基于式14.1的条件独立性，隐马尔可夫模型的这三个问题均能被高效求解。



### 条件随机场

条件随机场(Conditional Random Field, CRF) 是一种判别式无向图模型。如前所述，生成式模型是直接对联合分布进行建模，而判别式模型则是对条件分布进行建模。隐马尔可夫模型属于生成式模型，而条件随机场则是判别式模型。

条件随机场试图对多个变量在给定观测值后的条件概率进行建模。具体来说，如果令 $\mathbf{x} = \{x_1, \ldots, x_n \}$ 为观测序列， $\mathbf{y} = \{y_1, \ldots, y_n \}$ 为与之相对应的标记序列，则条件随机场的目标是构建条件概率模型 $P(\bf{y} | x)$ 。

... to be contiune